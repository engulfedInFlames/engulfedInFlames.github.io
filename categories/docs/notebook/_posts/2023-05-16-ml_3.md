---
title: ML 기본 다잡기 (3) &#58; 텐서를 조작해보자
tags: [TF]
sidebar:
  nav: categories
permalink: "/categories/docs/notebook/ml_3/"
article_header:
  type: cover
  image:
    src: /assets/images/logo/tf-banner.png
---

<div class="article__content" markdown="1">

&ensp; 지난 포스트에서 텐서 플로우가 무엇인지, 텐서 플로우에서 사용하는 텐서가 무엇인지를 알아보고, 텐서를 만들어보기까지를 다뤘습니다. 이번 포스트에서는 본격적으로 텐서 플로우의 갖가지 메소드들을 다루겠습니다.

---

### (1) 다양한 텐서들을 만들어보자

##### 랜덤 텐서 생성하기

&ensp; 랜덤 텐서란 임의의 숫자들을 포함하는 임의의 크기의 텐서를 의미합니다. `random` 모듈을 사용하여 손쉽게 랜덤 텐서를 생성할 수 있습니다.

```python
random_tensor1 = tf.random.Generator.from_seed(42)
random_tensor1 = random_tensor1.normal(shape=(3,2))
random_tensor2 = tf.random.Generator.from_seed(42)
random_tensor2 = random_tensor2.normal(shape=(3,2))

random_1, random_2, random_1 == random_2
```

&ensp; 출력 결과는 다음과 같습니다.

```
(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[-0.7565803 , -0.06854702],
        [ 0.07595026, -1.2573844 ],
        [-0.23193763, -1.8107855 ]], dtype=float32)>,
 <tf.Tensor: shape=(3, 2), dtype=float32, numpy=
 array([[-0.7565803 , -0.06854702],
        [ 0.07595026, -1.2573844 ],
        [-0.23193763, -1.8107855 ]], dtype=float32)>,
 <tf.Tensor: shape=(3, 2), dtype=bool, numpy=
 array([[ True,  True],
        [ True,  True],
        [ True,  True]])>)
```

&ensp; `random_tensor1`의 출력 결과를 보면, 수를 지정하지 않아도 랜덤한 부동 소수점 수들이 요소로 들어가 있는 것을 확인할 수 있습니다. 그런데 분명히 랜덤하게 생성했음에도 `random_tensor1`과 `random_tensor2`의 출력 결과가 같습니다. 왜일까요?

##### 시드(Seed)

&ensp; 시드란 일종의 파라미터로, 텐서 플로우는 두 개의 시드를 조합하여 이를 기반으로 랜덤한 텐서를 생성합니다. 두 개의 시드란 하나는 전역 수준 시드(global seed), 다른 하나는 연산 수준 시드(operation seed)를 의미합니다.

&ensp; 위에서는 `from_seed` 메소드로 global seed를 지정했습니다. 이를 배열에 비유하면 2차원 배열(seed)과 같다고 생각할 수 있습니다.

- 0번째 인덱스는 global seed를 나타낸다.
- 1번째 인덱스는 operation seed를 나타낸다.

&ensp; 위에서 생성한 `random_tensor1`을 `seed[42][oper_seed]`의 결과값이라 생각하면 쉽게 이해할 수 있습니다. (물론 실제로 그렇지는 않습니다.)

&ensp; 위에서 `random_tensor1`과 `random_tensor2` 둘 모두 `from_seed` 메소드에 의해 42라는 global seed가 설정됐습니다. 그래서 임의의 텐서라 할지라도 출발점은 항상 같다고 이해할 수 있습니다. 도착점을 결정하는 것이 바로 operation seed입니다. global seed도, operation seed도 값을 지정하지 않으면, 시스템이나 커널이 실행될 때 임의로 결정되는 값들을 사용하게 됩니다.

&ensp; `random_tensor1`이 실행될 때 시스템 역시 임의의 operation seed를 선언합니다. 그래서 `random_tensor1`과 `random_tensor2`의 도착점을 지정하지 않더라도, 시스템이 자신만의 operation seed를 사용하기 때문에 같은 결과가 출력되는 것입니다.

&ensp; 시드를 사용하는 이유는 임의의 텐서를 생성하더라도 재생산할 수 있는 결과를 얻기 위함입니다. 시드에 대해 더 자세히 알고 싶다면 다음의 문서를 참조하세요.

[텐서 플로우](https://www.tensorflow.org/api_docs/python/tf/random/set_seed)

<br/>

### (2) 텐서를 섞어보자

&ensp; 생성된 텐서를 섞을 때는 `shuffle` 메소드를 사용할 수 있습니다.

```python
not_shuffled = tf.constant([[10,7],
                           [3,4],
                           [2,5]])
tf.random.shuffle(not_shuffled, seed=42)
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 3,  4],
       [10,  7],
       [ 2,  5]], dtype=int32)>
```

&ensp; 섞이는 순서 역시 시드의 조합으로 나타낼 수 있습니다.

### (3) 다양한 방법으로 텐서를 만들어보자

&ensp; 요소가 전부 1, 또는 0인 텐서를 생성할 수도 있습니다. 이때는 각각 `ones`, `zeros` 메소드를 사용합니다.

```python
tf.ones(shape=(2,7,3))
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(2, 7, 3), dtype=float32, numpy=
array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]],

       [[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]]], dtype=float32)>
```

```python
tf.zeros(shape=(3,4))
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]], dtype=float32)>
```

<br/>

### (4) NumPy 리스트를 텐서로 변환해보자

&ensp; 텐서와 마찬가지로 NumPy 리스트도 수를 요소로 갖는 리스트입니다. 텐서 플로우의 텐서와 NumPy의 리스트 상호작용이 가능합니다.

```python
import numpy as np

numpy_list = np.range(1, 25, dtype=np.int32)
a_tensor = tf.constant(numpy_list, shape=(3,8))

a_tensor, a_tensor.size
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(3, 8), dtype=int32, numpy=
array([[ 1,  2,  3,  4,  5,  6,  7,  8],
       [ 9, 10, 11, 12, 13, 14, 15, 16],
       [17, 18, 19, 20, 21, 22, 23, 24]], dtype=int32)>,

24
```

&ensp; 유사 텐서 객체(Tensor like object)를 텐서로 변환할 때는 shape을 잘 지정해야 합니다. numpy_list는 길이가 24이므로, 텐서의 크기 역시 24로 동일해야 합니다.

<br/>

### (4) 텐서를 인덱싱해보자

##### 텐서 슬라이싱

&ensp; 텐서의 인덱스는 기본적으로 파이썬의 리스트와 같은 방식으로 접근할 수 있습니다.

```python
tensor = tf.ones(shape=(2,3,6))

# tensopr = [[[0., 0., 0., 0., 0., 0.],
#             [0., 0., 0., 0., 0., 0.],
#             [0., 0., 0., 0., 0., 0.]],

#             [[0., 0., 0., 0., 0., 0.],
#              [0., 0., 0., 0., 0., 0.],
#              [0., 0., 0., 0., 0., 0.]]]

tensor[:1, :2, :3], tensor[:, :1, :1]
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[0., 0., 0.],
        [0., 0., 0.]]], dtype=float32)>

<tf.Tensor: shape=(2, 1, 1), dtype=float32, numpy=
array([[[0.]],
       [[0.]]], dtype=float32)>
```

&ensp; 출력 결과를 예상하기 어려울 때는 앞이 아닌 뒤에서부터 인덱싱해보는 것도 방법입니다.

<br/>

##### 텐서 차원 늘리기

&ensp; JS의 spread와 같은 방법으로 텐서의 차원을 늘릴 수 있습니다.

```python
rank_2_tensor = tf.constant([[2,4], [7,1]])
rank_3_tensor = rank_2_tensor[..., tf.newaxis]

rank_3_tensor
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=
array([[[2],
        [4]],

       [[7],
        [1]]], dtype=int32)>
```

&ensp; 하지만 이 방법보다 텐서 플로우의 `expand_dims` 메소드를 사용하는 것이 바람직합니다.

```python
rank_2_tensor = tf.constant([[2,4], [7,1]])
rank_3_tensor1 = tf.expand_dims(rank_2_tensor, axis=0)
rank_3_tensor2 = tf.expand_dims(rank_2_tensor, axis=1)

rank_3_tensor1, rank_3_tensor2
```

&ensp; 출력 결과는 다음과 같습니다.

```
<tf.Tensor: shape=(1, 2, 2), dtype=int32, numpy=
array([[[2, 4],
        [7, 1]]], dtype=int32)>,

<tf.Tensor: shape=(2, 1, 2), dtype=int32, numpy=
array([[[2, 4]],

       [[7, 1]]], dtype=int32)>
```

&ensp; `expand_dims`는 `axis` 키워드를 통해 텐서 내 어느 위치에서 차원을 확장시킬 것인지를 지정할 수 있습니다.

<br/>

### (5) 텐서를 연산해보자

##### 기본 연산

&ensp; 텐서의 기본 연산은 간단한 산수 표현으로도 수행할 수 있습니다.

```python
tensor = tf.constant([[10,7], [3,4]])
print(tensor+10, " / " , tensor-10)

# [[20, 17], [13, 14]] / [[ 0, -3], [-7, -6]]
```

&ensp; 하지만 연산 속도는 빌트인 메소드를 사용하는 것이 더 빠릅니다.

```python
tf.multiply(tensor, tensor)
tf.add(tensor, tensor)
```

##### 텐서곱

&ensp; 텐서를 곱할 때는 주의할 필요가 있습니다. 텐서를 곱할 때는 크게 `multiply`를 통한 곱셈과 `matmul`을 통한 곱셈 둘로 나눌 수 있습니다.

- `multiply`: 곱하는 두 텐서의 shape이 같아야 하며, 같은 인덱스에 위치한 요소들끼리 곱한 결과를 반환한다.

- `matmul`: 행렬 곱셈한 결과를 반환한다.

---

**출처**
[Learn TensorFlow and Deep Learning fundamentals with Python](https://www.youtube.com/watch?v=tpCFfeUEGs8&list=LL&index=26)

</div>
